---
layout:     post
title:      Hbase笔记
subtitle:   Hbase笔记
date:       2021-10-10
author:     AnAn
header-img: /img/post-bg-article.jpg
catalog: true
tags:
    - Hbase
    - Hadoop
---

### 介绍
以下学习笔记来自于尚硅谷的Hbase课程

### 目录
- [前言](#前言)
- [habse写数据](#habse写数据)
- [MemStore刷写时机](#MemStore刷写时机)
- [hbase和并与拆分](#hbase和并与拆分)


<a name="前言"></a>
### 前言
<a style="position:relative;">![hbase架构](/img/post/hbase-note/2021-10-12_15-13.png)</a>
- HMaster 记录元数据，管理表（DDL），DML任务分发给RegionServer
- HRegionServer 处理增删改查（DML)
- HRegion 一张表占用一个或多个HRegion，一个HRegion只从属于一张表
- Store对应一个列族,在同一个Region中不会有同名的Store（列族)
- StoreFile
  保存实际数据的物理文件， StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有
  一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。
- MemStore
  写缓存， 由于 HFile 中的数据要求是有序的， 所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的 HFile。
- WAL(Hlog 预写入日志，文件对应写入预写入缓存MemStore)
  由于数据要经 MemStore 排序后才能刷写到 HFile， 但把数据保存在内存中会有很高的
  概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件
  中，然后再写入 MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重
  建。
- Hbase客户端在读写数据时直接与ZK和RegionServer交互
- **Hbase是一个读比写慢的框架**  

<a name="habse写数据"></a>
### habse写数据
- put NSP:Stu,rowkey:0001,name:张三  
1. 向ZK请求hbase:meta表的位置（一个HregionServer，代号HR1）
2. 在HR1查找维护NSP:Stu表的HregionServer位置，代号HR2
3. 在HR2中存储数据，HR2存储成功后返回ACK  


<a name="MemStore刷写时机"></a>
### MemStore刷写时机  
1. 当某个 memstroe 的大小达到了 hbase.hregion.memstore.flush.size（默认值 128M） ，
其所在 region 的所有 memstore 都会刷写。
当 memstore 的大小达到了
hbase.hregion.memstore.flush.size（默认值 128M），hbase.hregion.memstore.block.multiplier（默认值 4）
时，会阻止继续往该 memstore 写数据。
2.  region server 中 memstore 的总大小达到
java_heapsize
*hbase.regionserver.global.memstore.size（默认值 0.4）
*hbase.regionserver.global.memstore.size.lower.limit（默认值 0.95） ，
region 会按照其所有 memstore 的大小顺序（由大到小）依次进行刷写。直到 region server
中所有 memstore 的总大小减小到上述值以下。
当 region server 中 memstore 的总大小达到
java_heapsize*hbase.regionserver.global.memstore.size（默认值 0.4）
时，会阻止继续往所有的 memstore 写数据。
3. 到达自动刷写的时间，也会触发 memstore flush。自动刷新的时间间隔由该属性进行
配置 hbase.regionserver.optionalcacheflushinterval（默认 1 小时） 。尚硅谷大数据技术之 Hbase  
4. 当 WAL 文件的数量超过 hbase.regionserver.max.logs， region 会按照时间顺序依次进
行刷写，直到 WAL 文件数量减小到 hbase.regionserver.max.log 以下（该属性名已经废弃，
现无需手动设置， 最大值为 32）

<a name="hbase读数据"></a>
### hbase读数据  

![hbase读数据](/img/post/hbase-note/2021-10-12_15-14.png)

1. Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。
2. 访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey，
查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以
及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问。
3. 与目标 Region Server 进行通讯；
4. 分别在 Block Cache（读缓存）， MemStore 和 Store File（HFile）中查询目标数据，并将
查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不
同的类型（Put/Delete）。
5. 将从文件中查询到的数据块（Block， HFile 数据存储单元，默认大小为 64KB）缓存到
Block Cache。
6. 将合并后的最终结果返回给客户端。
7 **Block Cache用于缓存读取的磁盘的数据，如果Block Cache 种存在命中数据，则只需要合并Memstore和Block Cache中的数据**

<a name="hbase和并与拆分"></a>
### hbase和并数据(SotreFile Compaction) 和 拆分表(Region Split)  
- 合并  
  ![hbase合并数据](/img/post/hbase-note/2021-10-15_13-10.png)  
  - habeshell 1.minor compact:compact(简单合并) 2.major compact : major_compact(会删除过期版本数据)
  - 自动触发条件：
    1. habse.hregion.majorcompaction,默认值为7天，每隔7天来一次大合并，生产环境建议关闭
    2. habse.hstore.compactionThreshould 一个store中允许hfile的个数，达到时触发

- 拆分    
  ![hbase拆分数据](/img/post/hbase-note/2021-10-15_15-09.png)  
  - 默认情况下，每个 Table 起初只有一个 Region，随着数据的不断写入， Region 会自动进
  行拆分。刚拆分时，两个子 Region 都位于当前的 Region Server，但处于负载均衡的考虑，
  HMaster 有可能会将某个 Region 转移给其他的 Region Server。
  - Region Split 时机：
    1. 当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize，
    该 Region 就会进行拆分（0.94 版本之前）。
    2. 当 1 个 region 中 的 某 个 Store 下 所 有 StoreFile 的 总 大 小 超 过 Min(R^2 *
    "hbase.hregion.memstore.flush.size",hbase.hregion.max.filesize")， 该 Region 就会进行拆分，其
    中 R 为当前 Region Server 中属于该 Table 的个数（0.94 版本之后）。

- **总结**
  - habse和并的是族内数据，同一个store(列族)合并为一个hfile，合并可以提高数据的访问速度，保证数据整体的有序性。
  - hbase拆分拆分是将一个Region分裂成两个Region（同一个Region中的多个Store同步拆分），后期再将region分离到别的机器上（RegionServer）
  ，先合并再拆分可以保证了两个SotreFile数据的有序性,同时也兼顾了性能（小文件读取和写入更快），最终目的是实现负载均衡。
  - 生产中建议只使用一个列族，因为列族不均匀，数据量少的列族会形成大量小文件。